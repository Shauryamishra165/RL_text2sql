{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RL Text-to-SQL with GRPO on Spider\n\nTrains a Text-to-SQL model using GRPO (Group Relative Policy Optimization)\non the Spider benchmark.\n\n**Before running:** `Runtime > Change runtime type > T4 GPU`\n\n- Part A: Setup (install deps, create source files)\n- Part B: Pipeline test with dummy data\n- Part C: Train on Spider"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers==4.46.3 peft==0.13.2 bitsandbytes==0.44.1 accelerate==1.1.1 datasets sqlparse func-timeout pyyaml tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Create Source Files\n\nWrite all the project source files into a `src/` package for Colab."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/__init__.py\n",
    "# RL Text-to-SQL with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile src/sql_executor.py\n\"\"\"\nSafe SQL execution with timeout (threading-based).\n\"\"\"\nimport sqlite3\nimport threading\nfrom typing import Optional, Tuple, Any\n\n\nclass SQLTimeoutError(Exception):\n    pass\n\n\ndef _run_query(sql, db_path, result_holder):\n    \"\"\"Run a SQL query in a thread and store the result.\"\"\"\n    try:\n        conn = sqlite3.connect(db_path, timeout=5)\n        conn.text_factory = str\n        cursor = conn.cursor()\n        cursor.execute(sql)\n        result_holder[\"results\"] = cursor.fetchall()\n        result_holder[\"success\"] = True\n        conn.close()\n    except sqlite3.Error as e:\n        result_holder[\"error\"] = f\"SQLite error: {e}\"\n    except Exception as e:\n        result_holder[\"error\"] = f\"Execution error: {e}\"\n\n\nclass SQLExecutor:\n    def __init__(self, timeout=10):\n        self.timeout = timeout\n\n    def execute(self, sql, db_path):\n        if not sql or not sql.strip():\n            return False, None, \"Empty SQL query\"\n        result_holder = {\"success\": False, \"results\": None, \"error\": None}\n        t = threading.Thread(target=_run_query, args=(sql, db_path, result_holder))\n        t.start()\n        t.join(timeout=self.timeout)\n        if t.is_alive():\n            return False, None, f\"SQL execution timed out after {self.timeout}s\"\n        if result_holder[\"success\"]:\n            return True, result_holder[\"results\"], None\n        return False, None, result_holder.get(\"error\", \"Unknown error\")\n\n    def compare_results(self, pred_results, gold_results):\n        if pred_results is None or gold_results is None:\n            return False\n        try:\n            return set(tuple(r) for r in pred_results) == set(tuple(r) for r in gold_results)\n        except (TypeError, ValueError):\n            return pred_results == gold_results\n\n    def execute_and_compare(self, pred_sql, gold_sql, db_path):\n        pred_ok, pred_res, pred_err = self.execute(pred_sql, db_path)\n        gold_ok, gold_res, gold_err = self.execute(gold_sql, db_path)\n        match = False\n        if pred_ok and gold_ok:\n            match = self.compare_results(pred_res, gold_res)\n        return {\"pred_success\": pred_ok, \"gold_success\": gold_ok, \"execution_match\": match,\n                \"pred_error\": pred_err, \"pred_results\": pred_res, \"gold_results\": gold_res}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile src/reward.py\n\"\"\"\nMulti-component reward function (SQL-R1 style).\n\"\"\"\nimport re\nimport sqlparse\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom src.sql_executor import SQLExecutor\n\n\n@dataclass\nclass RewardConfig:\n    correct_execution: float = 1.0\n    valid_but_wrong: float = 0.1\n    invalid_sql: float = -0.5\n    format_bonus: float = 0.2\n    partial_match_bonus: float = 0.3\n    execution_timeout: int = 10\n\n\n@dataclass\nclass RewardResult:\n    total_reward: float\n    execution_correct: bool\n    sql_valid: bool\n    format_ok: bool\n    partial_match_score: float\n    error_message: Optional[str] = None\n\n\nclass RewardComputer:\n    def __init__(self, config: RewardConfig):\n        self.config = config\n        self.executor = SQLExecutor(timeout=config.execution_timeout)\n\n    def extract_sql_from_response(self, response):\n        m = re.search(r\"```sql\\s*(.*?)\\s*```\", response, re.DOTALL | re.IGNORECASE)\n        if m:\n            return m.group(1).strip()\n        m = re.search(r\"```\\s*(.*?)\\s*```\", response, re.DOTALL)\n        if m:\n            return m.group(1).strip()\n        m = re.search(r\"(SELECT\\s+.*?)(?:;|\\Z)\", response, re.DOTALL | re.IGNORECASE)\n        if m:\n            return m.group(1).strip()\n        return response.strip()\n\n    def check_format(self, response):\n        return bool(re.search(r\"```sql\\s*.*?\\s*```\", response, re.DOTALL | re.IGNORECASE))\n\n    def _extract_identifiers(self, parsed):\n        ids = []\n        for token in parsed.flatten():\n            if token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Name.Placeholder):\n                ids.append(token.value.lower())\n        return ids\n\n    def compute_partial_match(self, pred_sql, gold_sql):\n        try:\n            pred_parsed = sqlparse.parse(pred_sql)\n            gold_parsed = sqlparse.parse(gold_sql)\n            if not pred_parsed or not gold_parsed:\n                return 0.0\n            pred_tokens = set(self._extract_identifiers(pred_parsed[0]))\n            gold_tokens = set(self._extract_identifiers(gold_parsed[0]))\n            if not gold_tokens:\n                return 0.0\n            return len(pred_tokens & gold_tokens) / len(gold_tokens)\n        except Exception:\n            return 0.0\n\n    def compute_reward(self, response, gold_sql, db_path):\n        reward = 0.0\n        pred_sql = self.extract_sql_from_response(response)\n        format_ok = self.check_format(response)\n        if format_ok:\n            reward += self.config.format_bonus\n        exec_result = self.executor.execute_and_compare(pred_sql, gold_sql, db_path)\n        if exec_result[\"execution_match\"]:\n            reward += self.config.correct_execution\n            return RewardResult(total_reward=reward, execution_correct=True, sql_valid=True,\n                                format_ok=format_ok, partial_match_score=1.0)\n        if exec_result[\"pred_success\"]:\n            reward += self.config.valid_but_wrong\n            partial = self.compute_partial_match(pred_sql, gold_sql)\n            reward += self.config.partial_match_bonus * partial\n            return RewardResult(total_reward=reward, execution_correct=False, sql_valid=True,\n                                format_ok=format_ok, partial_match_score=partial)\n        reward += self.config.invalid_sql\n        return RewardResult(total_reward=reward, execution_correct=False, sql_valid=False,\n                            format_ok=format_ok, partial_match_score=0.0, error_message=exec_result[\"pred_error\"])\n\n    def compute_group_rewards(self, responses, gold_sql, db_path):\n        return [self.compute_reward(resp, gold_sql, db_path) for resp in responses]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile src/data_utils.py\n\"\"\"\nSpider dataset loading and prompt formatting.\n\"\"\"\nimport json\nimport os\nimport sqlite3\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Text2SQLSample:\n    question: str\n    gold_sql: str\n    db_id: str\n    db_path: str\n    schema: str\n\n\nSYSTEM_PROMPT = (\"You are an expert SQL assistant. Given a natural language question \"\n    \"and a database schema, generate the correct SQL query.\\n\\n\"\n    \"Rules:\\n- Output ONLY the SQL query wrapped in ```sql``` tags\\n\"\n    \"- Use proper SQL syntax for SQLite\\n- Do not include explanations outside the SQL block\")\n\n\ndef get_schema_from_db(db_path):\n    try:\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND sql IS NOT NULL\")\n        tables = cursor.fetchall()\n        conn.close()\n        return \"\\n\\n\".join(t[0] for t in tables if t[0])\n    except Exception as e:\n        return f\"-- Error reading schema: {e}\"\n\n\ndef format_prompt(question, schema):\n    return (f\"Given the following database schema:\\n\\n{schema}\\n\\n\"\n            f\"Question: {question}\\n\\nGenerate the SQL query. Wrap it in ```sql``` tags.\")\n\n\ndef build_chat_messages(question, schema):\n    return [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": format_prompt(question, schema)},\n    ]\n\n\ndef load_spider_dataset(data_file, db_dir, max_samples=None):\n    if not os.path.exists(data_file):\n        raise FileNotFoundError(f\"Spider data file not found: {data_file}\")\n    with open(data_file, \"r\") as f:\n        data = json.load(f)\n    samples = []\n    skipped = 0\n    for item in data:\n        db_id = item[\"db_id\"]\n        db_path = os.path.join(db_dir, db_id, f\"{db_id}.sqlite\")\n        if not os.path.exists(db_path):\n            skipped += 1\n            continue\n        schema = get_schema_from_db(db_path)\n        samples.append(Text2SQLSample(\n            question=item[\"question\"], gold_sql=item.get(\"query\", item.get(\"SQL\", \"\")),\n            db_id=db_id, db_path=db_path, schema=schema))\n        if max_samples and len(samples) >= max_samples:\n            break\n    if skipped > 0:\n        print(f\"Warning: Skipped {skipped} samples (missing db files)\")\n    print(f\"Loaded {len(samples)} samples from {data_file}\")\n    return samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile src/model_utils.py\n\"\"\"\nModel loading with QLoRA (float16 for T4 compatibility).\n\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n\n\ndef load_tokenizer(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    return tokenizer\n\n\ndef load_model_with_lora(model_name, lora_r=16, lora_alpha=32, lora_dropout=0.05,\n                         target_modules=None, quantization=\"4bit\"):\n    if target_modules is None:\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    bnb_config = None\n    if quantization == \"4bit\":\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, quantization_config=bnb_config, torch_dtype=torch.float16,\n        device_map=\"auto\", trust_remote_code=True)\n    if quantization in (\"4bit\", \"8bit\"):\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n    lora_config = LoraConfig(r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n        target_modules=target_modules, task_type=TaskType.CAUSAL_LM, bias=\"none\")\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    return model\n\n\ndef create_reference_model(model):\n    ref_params = {}\n    for name, param in model.named_parameters():\n        if \"lora_\" in name:\n            ref_params[name] = param.data.clone().detach()\n    print(f\"Reference policy: {len(ref_params)} LoRA tensors frozen\")\n    return ref_params"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile src/grpo_trainer.py\n\"\"\"\nGRPO trainer for Text-to-SQL.\nGenerates K candidates, executes them, uses group-normalized advantages.\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\nfrom src.reward import RewardComputer, RewardConfig, RewardResult\nfrom src.data_utils import Text2SQLSample, build_chat_messages\n\n\n@dataclass\nclass GRPOConfig:\n    group_size: int = 4\n    clip_epsilon: float = 0.2\n    kl_coeff: float = 0.05\n    temperature: float = 0.7\n    max_new_tokens: int = 256\n    top_p: float = 0.95\n\n\nclass GRPOTrainer:\n    def __init__(self, model, tokenizer, ref_params, optimizer, grpo_config, reward_config, device=\"cuda\"):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.ref_params = ref_params\n        self.optimizer = optimizer\n        self.config = grpo_config\n        self.reward_computer = RewardComputer(reward_config)\n        self.device = device\n        self.global_step = 0\n\n    @torch.no_grad()\n    def generate_group(self, sample):\n        \"\"\"Generate K SQL candidates one at a time.\"\"\"\n        messages = build_chat_messages(sample.question, sample.schema)\n        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n        prompt_length = inputs[\"input_ids\"].shape[1]\n        self.model.eval()\n        all_responses, all_token_log_probs, all_generated_ids = [], [], []\n        for k in range(self.config.group_size):\n            outputs = self.model.generate(\n                **inputs, max_new_tokens=self.config.max_new_tokens, do_sample=True,\n                temperature=self.config.temperature, top_p=self.config.top_p,\n                num_return_sequences=1, return_dict_in_generate=True, output_scores=True,\n                pad_token_id=self.tokenizer.pad_token_id)\n            gen_ids = outputs.sequences[:, prompt_length:]\n            scores = torch.stack(outputs.scores, dim=1)\n            log_probs_all = F.log_softmax(scores, dim=-1)\n            gen_len = gen_ids.shape[1]\n            log_probs_all = log_probs_all[:, :gen_len, :]\n            token_lp = torch.gather(log_probs_all, 2, gen_ids.unsqueeze(-1)).squeeze(-1)\n            response = self.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n            all_responses.append(response)\n            all_token_log_probs.append(token_lp.squeeze(0))\n            all_generated_ids.append(gen_ids.squeeze(0))\n        max_len = max(t.shape[0] for t in all_generated_ids)\n        padded_log_probs = torch.zeros(self.config.group_size, max_len, device=self.device)\n        padded_gen_ids = torch.full((self.config.group_size, max_len),\n            self.tokenizer.pad_token_id, device=self.device, dtype=all_generated_ids[0].dtype)\n        for k in range(self.config.group_size):\n            length = all_generated_ids[k].shape[0]\n            padded_gen_ids[k, :length] = all_generated_ids[k]\n            padded_log_probs[k, :length] = all_token_log_probs[k]\n        return all_responses, padded_log_probs, padded_gen_ids\n\n    def compute_advantages(self, rewards):\n        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n        return ((rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)).to(self.device)\n\n    def compute_kl_penalty(self):\n        kl = torch.tensor(0.0, device=self.device)\n        count = 0\n        for name, param in self.model.named_parameters():\n            if name in self.ref_params:\n                kl += F.mse_loss(param, self.ref_params[name].to(param.device), reduction=\"sum\")\n                count += 1\n        return kl / max(count, 1)\n\n    def compute_policy_loss(self, sample, responses, old_log_probs, generated_ids, advantages):\n        self.model.train()\n        messages = build_chat_messages(sample.question, sample.schema)\n        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)[\"input_ids\"].to(self.device)\n        prompt_len = prompt_ids.shape[1]\n        total_loss = torch.tensor(0.0, device=self.device)\n        valid_samples = 0\n        for k in range(self.config.group_size):\n            gen_ids_k = generated_ids[k].unsqueeze(0)\n            pad_mask = gen_ids_k[0] != self.tokenizer.pad_token_id\n            gen_ids_k = gen_ids_k[:, pad_mask]\n            if gen_ids_k.shape[1] == 0:\n                continue\n            full_ids = torch.cat([prompt_ids, gen_ids_k], dim=1)\n            outputs = self.model(full_ids, use_cache=False)\n            gen_logits = outputs.logits[:, prompt_len - 1:-1, :]\n            new_log_probs = F.log_softmax(gen_logits, dim=-1)\n            gen_len = min(gen_ids_k.shape[1], new_log_probs.shape[1])\n            new_token_lp = torch.gather(new_log_probs[:, :gen_len, :], 2, gen_ids_k[:, :gen_len].unsqueeze(-1)).squeeze(-1)\n            old_token_lp = old_log_probs[k, :gen_len].unsqueeze(0)\n            ratio = torch.exp(new_token_lp - old_token_lp.detach())\n            avg_ratio = ratio.mean()\n            adv = advantages[k]\n            surr1 = avg_ratio * adv\n            surr2 = torch.clamp(avg_ratio, 1.0 - self.config.clip_epsilon, 1.0 + self.config.clip_epsilon) * adv\n            total_loss += -torch.min(surr1, surr2)\n            valid_samples += 1\n        if valid_samples > 0:\n            total_loss = total_loss / valid_samples\n        kl_penalty = self.compute_kl_penalty()\n        loss = total_loss + self.config.kl_coeff * kl_penalty\n        return loss, {\"policy_loss\": total_loss.item(), \"kl_penalty\": kl_penalty.item(),\n                      \"total_loss\": loss.item(), \"mean_advantage\": advantages.mean().item()}\n\n    def train_step(self, sample):\n        responses, old_log_probs, generated_ids = self.generate_group(sample)\n        reward_results = self.reward_computer.compute_group_rewards(responses, sample.gold_sql, sample.db_path)\n        rewards = [r.total_reward for r in reward_results]\n        advantages = self.compute_advantages(rewards)\n        loss, metrics = self.compute_policy_loss(sample, responses, old_log_probs, generated_ids, advantages)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        n_correct = sum(1 for r in reward_results if r.execution_correct)\n        n_valid = sum(1 for r in reward_results if r.sql_valid)\n        metrics.update({\"mean_reward\": sum(rewards)/len(rewards), \"max_reward\": max(rewards),\n                        \"correct_ratio\": n_correct/len(rewards), \"valid_ratio\": n_valid/len(rewards),\n                        \"step\": self.global_step})\n        self.global_step += 1\n        return metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part B: Pipeline Test (Dummy Data)\n\nQuick check that the GRPO loop works before downloading Spider."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from src.model_utils import load_model_with_lora, load_tokenizer, create_reference_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = load_tokenizer(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model with QLoRA (4-bit)...\")\n",
    "torch.cuda.empty_cache()\n",
    "model = load_model_with_lora(model_name=MODEL_NAME, lora_r=16, lora_alpha=32, lora_dropout=0.05, quantization=\"4bit\")\n",
    "\n",
    "print(\"\\nCreating reference policy...\")\n",
    "ref_params = create_reference_model(model)\n",
    "\n",
    "print(f\"\\nGPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create dummy test data for pipeline verification\nimport sqlite3, tempfile\nfrom src.data_utils import Text2SQLSample, get_schema_from_db\n\ntmp_dir = tempfile.mkdtemp()\ntest_db_path = f\"{tmp_dir}/test.sqlite\"\n\nconn = sqlite3.connect(test_db_path)\nc = conn.cursor()\nc.execute(\"CREATE TABLE employees (id INTEGER PRIMARY KEY, name TEXT, department TEXT, salary REAL)\")\nc.execute(\"CREATE TABLE departments (id INTEGER PRIMARY KEY, name TEXT, budget REAL)\")\nc.executemany(\"INSERT INTO employees VALUES (?,?,?,?)\", [\n    (1,\"Alice\",\"Engineering\",120000), (2,\"Bob\",\"Marketing\",90000),\n    (3,\"Charlie\",\"Engineering\",110000), (4,\"Diana\",\"HR\",95000), (5,\"Eve\",\"Engineering\",130000)])\nc.executemany(\"INSERT INTO departments VALUES (?,?,?)\", [\n    (1,\"Engineering\",500000), (2,\"Marketing\",200000), (3,\"HR\",150000)])\nconn.commit()\nconn.close()\n\nschema = get_schema_from_db(test_db_path)\ntest_samples = [\n    Text2SQLSample(\"How many employees are there?\", \"SELECT COUNT(*) FROM employees\", \"test\", test_db_path, schema),\n    Text2SQLSample(\"What is the average salary?\", \"SELECT AVG(salary) FROM employees\", \"test\", test_db_path, schema),\n    Text2SQLSample(\"List employees in Engineering.\", \"SELECT name FROM employees WHERE department='Engineering'\", \"test\", test_db_path, schema),\n    Text2SQLSample(\"What is the highest salary?\", \"SELECT MAX(salary) FROM employees\", \"test\", test_db_path, schema),\n    Text2SQLSample(\"How many departments are there?\", \"SELECT COUNT(*) FROM departments\", \"test\", test_db_path, schema),\n]\nprint(f\"Created {len(test_samples)} test samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run 3 GRPO training steps on dummy data\nfrom src.grpo_trainer import GRPOTrainer, GRPOConfig\nfrom src.reward import RewardConfig\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6, weight_decay=0.01)\ntrainer = GRPOTrainer(\n    model=model, tokenizer=tokenizer, ref_params=ref_params, optimizer=optimizer,\n    grpo_config=GRPOConfig(group_size=4, max_new_tokens=256),\n    reward_config=RewardConfig(), device=\"cuda\")\n\nprint(\"Running 3 GRPO steps...\\n\")\nfor i, sample in enumerate(test_samples[:3]):\n    print(f\"Step {i+1}/3 | Q: '{sample.question}'\")\n    metrics = trainer.train_step(sample)\n    print(f\"  Loss={metrics['total_loss']:.4f}  Reward={metrics['mean_reward']:.3f}  \"\n          f\"Correct={metrics['correct_ratio']:.0%}  Valid={metrics['valid_ratio']:.0%}\\n\")\n\nprint(\"Pipeline test passed. Ready for Spider.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part C: Train on Spider\n\nDownload the dataset and run actual training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download Spider dataset\n!pip install -q datasets\nfrom datasets import load_dataset\nimport json, os, shutil\n\nprint(\"Downloading Spider from HuggingFace...\")\nds = load_dataset(\"xlangai/spider\")\n\nos.makedirs(\"data/spider\", exist_ok=True)\n\ntrain_records = [{\"question\": r[\"question\"], \"query\": r[\"query\"], \"db_id\": r[\"db_id\"]} for r in ds[\"train\"]]\nwith open(\"data/spider/train_spider.json\", \"w\") as f:\n    json.dump(train_records, f)\nprint(f\"Saved {len(train_records)} training samples\")\n\ndev_records = [{\"question\": r[\"question\"], \"query\": r[\"query\"], \"db_id\": r[\"db_id\"]} for r in ds[\"validation\"]]\nwith open(\"data/spider/dev.json\", \"w\") as f:\n    json.dump(dev_records, f)\nprint(f\"Saved {len(dev_records)} dev samples\")\n\nprint(\"\\nNote: You still need the SQLite database files.\")\nprint(\"Download from https://yale-lily.github.io/spider and upload the database/ folder.\")\nprint(\"Or run the next cell to try downloading them.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download Spider database files\nimport subprocess\n\nif not os.path.exists(\"data/spider/database\"):\n    print(\"Downloading Spider databases...\")\n    !wget -q https://drive.usercontent.google.com/download?id=1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J -O spider_dbs.zip 2>/dev/null || echo \"Direct download failed, trying gdown...\"\n    \n    if os.path.exists(\"spider_dbs.zip\"):\n        !unzip -q spider_dbs.zip -d data/spider/\n        print(\"Databases downloaded and extracted\")\n    else:\n        !pip install -q gdown\n        !gdown --id 1403EGqzIDoHMdQF4c9Bkyl7dZLZ5Wt6J -O spider_dbs.zip\n        if os.path.exists(\"spider_dbs.zip\"):\n            !unzip -q spider_dbs.zip -d data/spider/\n            print(\"Databases downloaded and extracted\")\n        else:\n            print(\"Could not auto-download databases.\")\n            print(\"Please download manually from https://yale-lily.github.io/spider\")\n            print(\"Upload the 'database' folder to data/spider/database/\")\nelse:\n    num_dbs = len(os.listdir(\"data/spider/database\"))\n    print(f\"Spider databases already present: {num_dbs} databases\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spider dataset\n",
    "from src.data_utils import load_spider_dataset\n",
    "\n",
    "# Start with 100 samples for quick iteration (remove max_samples for full training)\n",
    "MAX_TRAIN = 100\n",
    "MAX_EVAL = 50\n",
    "\n",
    "train_data = load_spider_dataset(\"data/spider/train_spider.json\", \"data/spider/database\", max_samples=MAX_TRAIN)\n",
    "eval_data = load_spider_dataset(\"data/spider/dev.json\", \"data/spider/database\", max_samples=MAX_EVAL)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_data)} samples\")\n",
    "print(f\"Eval:  {len(eval_data)} samples\")\n",
    "print(f\"\\nExample: [{train_data[0].db_id}] {train_data[0].question}\")\n",
    "print(f\"  Gold: {train_data[0].gold_sql}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate model BEFORE RL training (baseline)\nfrom src.reward import RewardComputer, RewardConfig\nfrom src.data_utils import build_chat_messages\n\nmodel.eval()\nreward_computer = RewardComputer(RewardConfig())\ncorrect_before = 0\n\nprint(\"Evaluating baseline (before RL)...\\n\")\nfor sample in eval_data[:20]:\n    messages = build_chat_messages(sample.question, sample.schema)\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=256, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n    response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    r = reward_computer.compute_reward(response, sample.gold_sql, sample.db_path)\n    correct_before += int(r.execution_correct)\n    status = \"PASS\" if r.execution_correct else \"FAIL\"\n    print(f\"  [{status}] [{sample.db_id}] {sample.question[:60]}\")\n\nbaseline_acc = correct_before / 20\nprint(f\"\\nBaseline accuracy: {correct_before}/20 = {baseline_acc:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Training on Spider\n",
    "import random\n",
    "from src.grpo_trainer import GRPOTrainer, GRPOConfig\n",
    "\n",
    "# Reset optimizer for fresh training\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model, tokenizer=tokenizer, ref_params=ref_params, optimizer=optimizer,\n",
    "    grpo_config=GRPOConfig(group_size=4, clip_epsilon=0.2, kl_coeff=0.05, temperature=0.7, max_new_tokens=256),\n",
    "    reward_config=RewardConfig(), device=\"cuda\")\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "all_metrics = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch+1}/{NUM_EPOCHS} â€” Training on {len(train_data)} Spider samples\")\n",
    "    print(f\"{'='*60}\")\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    for step, sample in enumerate(train_data):\n",
    "        print(f\"\\nStep {step+1}/{len(train_data)} | [{sample.db_id}] {sample.question[:50]}...\")\n",
    "        metrics = trainer.train_step(sample)\n",
    "        all_metrics.append(metrics)\n",
    "        print(f\"  Loss={metrics['total_loss']:.4f}  Reward={metrics['mean_reward']:.3f}  \"\n",
    "              f\"Correct={metrics['correct_ratio']:.0%}  Valid={metrics['valid_ratio']:.0%}\")\n",
    "\n",
    "    avg_reward = sum(m['mean_reward'] for m in all_metrics[-len(train_data):]) / len(train_data)\n",
    "    avg_correct = sum(m['correct_ratio'] for m in all_metrics[-len(train_data):]) / len(train_data)\n",
    "    print(f\"\\nEpoch {epoch+1} summary: avg_reward={avg_reward:.3f}, avg_correct={avg_correct:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "steps = range(len(all_metrics))\n",
    "\n",
    "axes[0].plot(steps, [m['total_loss'] for m in all_metrics], 'b-', alpha=0.7)\n",
    "axes[0].set_title('Total Loss'); axes[0].set_xlabel('Step'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(steps, [m['mean_reward'] for m in all_metrics], 'g-', alpha=0.7)\n",
    "axes[1].set_title('Mean Reward'); axes[1].set_xlabel('Step'); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(steps, [m['correct_ratio'] for m in all_metrics], 'r-', alpha=0.7)\n",
    "axes[2].set_title('Execution Correct Ratio'); axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylim(-0.05, 1.05); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/spider_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate AFTER RL training\nmodel.eval()\ncorrect_after = 0\n\nprint(\"Evaluating after GRPO training...\\n\")\nfor sample in eval_data[:20]:\n    messages = build_chat_messages(sample.question, sample.schema)\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=256, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n    response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    r = reward_computer.compute_reward(response, sample.gold_sql, sample.db_path)\n    correct_after += int(r.execution_correct)\n    status = \"PASS\" if r.execution_correct else \"FAIL\"\n    pred_sql = reward_computer.extract_sql_from_response(response)\n    print(f\"  [{status}] [{sample.db_id}] {sample.question[:50]}\")\n    print(f\"      Gold: {sample.gold_sql}\")\n    print(f\"      Pred: {pred_sql}\")\n\npost_acc = correct_after / 20\nprint(f\"\\n{'='*50}\")\nprint(f\"Before RL: {baseline_acc:.0%}\")\nprint(f\"After RL:  {post_acc:.0%}\")\nprint(f\"Improvement: {post_acc - baseline_acc:+.0%}\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"outputs/grpo_spider_model\")\n",
    "tokenizer.save_pretrained(\"outputs/grpo_spider_model\")\n",
    "print(\"Model saved to outputs/grpo_spider_model/\")"
   ]
  }
 ]
}